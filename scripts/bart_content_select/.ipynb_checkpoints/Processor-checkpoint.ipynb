{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291c4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BartTokenizer, BartForCausalLM, BartForConditionalGeneration, BeamSearchScorer, LogitsProcessorList, MinLengthLogitsProcessor, TopKLogitsWarper, TemperatureLogitsWarper, BartModel\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import BartTokenizer, BartForCausalLM, BartForConditionalGeneration, BeamSearchScorer, LogitsProcessorList, MinLengthLogitsProcessor, TopKLogitsWarper, TemperatureLogitsWarper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d4d167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(tokenizer, df, surface_keys, content_keys, targets, max_length=1024, pad_to_max_length=True, return_tensors=\"pt\"):\n",
    "\n",
    "    \n",
    "    encoded_sentences = {}\n",
    "\n",
    "    target_ids = []\n",
    "    \n",
    "    def run_bart(snippet):\n",
    "        encoded_dict = tokenizer(\n",
    "          snippet,\n",
    "          max_length=max_length,\n",
    "          padding=\"max_length\" if pad_to_max_length else None,\n",
    "          truncation=True,\n",
    "          return_tensors=return_tensors,\n",
    "          add_prefix_space = True\n",
    "        )\n",
    "        return encoded_dict\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        content_val = []\n",
    "        num_items = len(eval(row[content_keys[0]]))\n",
    "        row_contents = {}\n",
    "        for idx in range(0, num_items):\n",
    "            idx_vals_content = [\"<%s> \"%k+ eval(row[k])[idx] + \" </%s>\"%k for k in content_keys]\n",
    "            content_sent = \" \".join(idx_vals_content)\n",
    "            content_val.append( \"<content> \"+ content_sent + \" </content>\")\n",
    "\n",
    "        for surface_k in surface_keys:\n",
    "            row_surface_k = eval(row[surface_k])\n",
    "            row_surface_k = \" \".join([\"<surface> <%s> \"%surface_k+ each + \" </%s> </surface>\"%surface_k for each in row_surface_k])\n",
    "            row_contents[surface_k] =  row_surface_k\n",
    "\n",
    "        row_content_sent = \" \".join(content_val)\n",
    "        row_contents['content'] = row_content_sent\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for key, key_sent in row_contents.items():\n",
    "            id_key = '%s_ids'%key\n",
    "            attention_mask_key = '%s_attention_masks'%key\n",
    "            if id_key not in encoded_sentences:\n",
    "                encoded_sentences[id_key] = []\n",
    "                encoded_sentences[attention_mask_key] = []\n",
    "                \n",
    "            sentence_encoding = run_bart(key_sent.strip())\n",
    "            \n",
    "            encoded_sentences[id_key].append(sentence_encoding['input_ids'])\n",
    "            encoded_sentences[attention_mask_key].append(sentence_encoding['attention_mask'])\n",
    "            \n",
    "    for tgt_sentence in targets:\n",
    "        encoded_dict = tokenizer(\n",
    "              tgt_sentence,\n",
    "              max_length=max_length,\n",
    "              padding=\"max_length\" if pad_to_max_length else None,\n",
    "              truncation=True,\n",
    "              return_tensors=return_tensors,\n",
    "              add_prefix_space = True\n",
    "        )\n",
    "        # Shift the target ids to the right\n",
    "        #shifted_target_ids = shift_tokens_right(encoded_dict['input_ids'], tokenizer.pad_token_id)\n",
    "        target_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "    for key in list(encoded_sentences.keys()):\n",
    "        print(len(encoded_sentences[key]))\n",
    "        encoded_sentences[key] = torch.cat(encoded_sentences[key], dim = 0)\n",
    "        print(encoded_sentences[key].shape)\n",
    "        \n",
    "    target_ids = torch.cat(target_ids, dim = 0)\n",
    "    encoded_sentences['labels'] = target_ids\n",
    "    print(encoded_sentences['labels'].shape)\n",
    "        \n",
    "        \n",
    "\n",
    "    return encoded_sentences\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0133f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, keys):\n",
    "    for key in keys:\n",
    "        df = df[df[key] != \"['']\"]\n",
    "    return df\n",
    "\n",
    "class SummaryDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, data_files, batch_size, num_examples = 20000 , max_len = 1024, flatten_studies = False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_files = data_files\n",
    "        self.batch_size = batch_size\n",
    "        self.num_examples = num_examples\n",
    "        self.max_len = max_len\n",
    "        self.flatten_studies = flatten_studies\n",
    "\n",
    "    # Loads and splits the data into training, validation and test sets with a 60/20/20 split\n",
    "    def prepare_data(self):\n",
    "        self.train = pd.read_csv(self.data_files[0])\n",
    "        self.validate = pd.read_csv(self.data_files[1])\n",
    "        self.test = pd.read_csv(self.data_files[2])\n",
    "        preprocess_keys = ['population', 'interventions', 'outcomes', 'SummaryConclusions','punchline_text', 'punchline_effect' ]\n",
    "        self.train = preprocess_df(self.train, preprocess_keys)\n",
    "        self.validate = preprocess_df(self.validate, preprocess_keys)\n",
    "        self.test = preprocess_df(self.test, preprocess_keys)\n",
    "\n",
    "    def setup(self):\n",
    "        self.train = encode_sentences(self.tokenizer, \n",
    "                                      self.train,\n",
    "                                        ['population', 'punchline_text'],\n",
    "                                        ['interventions_mesh', 'outcomes_mesh', 'punchline_effect'], \n",
    "                                        self.train['SummaryConclusions'],\n",
    "                                        max_length = self.max_len)\n",
    "        \n",
    "        self.validate = encode_sentences(self.tokenizer, \n",
    "                                      self.validate,\n",
    "                                        ['population', 'punchline_text'],\n",
    "                                        ['interventions_mesh', 'outcomes_mesh', 'punchline_effect'], \n",
    "                                        self.validate['SummaryConclusions'],\n",
    "                                        max_length = self.max_len)\n",
    "        self.test = encode_sentences(self.tokenizer, \n",
    "                                      self.test,\n",
    "                                        ['population', 'punchline_text'],\n",
    "                                        ['interventions_mesh', 'outcomes_mesh', 'punchline_effect'], \n",
    "                                        self.test['SummaryConclusions'],\n",
    "                                        max_length = self.max_len)\n",
    "        \n",
    "    def train_dataloader(self, data_type = 'robo'):\n",
    "        #dataset = TensorDataset\n",
    "        dataset = TensorDataset(self.train['content_ids'], self.train['content_attention_masks'],\n",
    "                                self.train['population_ids'], self.train['population_attention_masks'],\n",
    "                                self.train['punchline_text_ids'], self.train['punchline_text_attention_masks'],\n",
    "                                    self.train['labels'])\n",
    "        #dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n",
    "        train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n",
    "        return train_data\n",
    "\n",
    "    def val_dataloader(self, data_type = 'robo'):\n",
    "        dataset = TensorDataset(self.validate['content_ids'], self.validate['content_attention_masks'],\n",
    "                                self.validate['population_ids'], self.validate['population_attention_masks'],\n",
    "                                self.validate['punchline_text_ids'], self.validate['punchline_text_attention_masks'],\n",
    "                                    self.validate['labels'])\n",
    "        val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n",
    "        return val_data\n",
    "\n",
    "    def test_dataloader(self, data_type = 'robo'):\n",
    "        #print(self.test['punchline_text_ids'])\n",
    "        dataset = TensorDataset(self.test['content_ids'], self.test['content_attention_masks'],\n",
    "                                self.test['population_ids'], self.test['population_attention_masks'],\n",
    "                                self.test['punchline_text_ids'], self.test['punchline_text_attention_masks'],\n",
    "                                    self.train['labels'])\n",
    "        test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fc7c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sanjana/summarization/datasets/train_rr_data.csv\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([3235, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([406, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "5\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([406, 1024])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-5f30a40b8de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#print(summary_data.train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msummary_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-e45e7f4ae187>\u001b[0m in \u001b[0;36mval_dataloader\u001b[0;34m(self, data_type)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'population_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'population_attention_masks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'punchline_text_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'punchline_text_attention_masks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                     self.validate['labels'])\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size mismatch between tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "def make_data(tokenizer, SummaryDataModule,  data_type = 'robo', path = '/Users/sanjana', files = ['robo_train_sep.csv', 'robo_dev_sep.csv', 'robo_test_sep.csv'], max_len = 256):\n",
    "    if data_type == 'robo':\n",
    "        train_file = path + '/summarization/datasets/%s'%(files[0])\n",
    "        dev_file = path + '/summarization/datasets/%s'%(files[1])\n",
    "        test_file = path + '/summarization/datasets/%s'%(files[2])\n",
    "\n",
    "    print(train_file)\n",
    "    data_files = [train_file, dev_file, test_file]\n",
    "    summary_data = SummaryDataModule(tokenizer, data_files = data_files,  batch_size = 1, max_len = max_len, flatten_studies = True)\n",
    "    summary_data.prepare_data()\n",
    "    \n",
    "    assert(len(summary_data.train) > 10)\n",
    "    return summary_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    additional_special_tokens = [\"<sep>\", \"<study>\", \"</study>\",\n",
    "            \"<outcomes_mesh>\", \"</outcomes_mesh>\",\n",
    "            \"<punchline_text>\", \"</punchline_text>\",\n",
    "            \"<population_mesh>\", \"</population_mesh>\",\n",
    "            \"<interventions_mesh>\", \"</interventions_mesh>\",\n",
    "            \"<punchline_effect>\", \"</punchline_effect>\"]\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', bos_token=\"<s>\", \n",
    "                                                    eos_token=\"</s>\", \n",
    "                                                    pad_token = \"<pad>\")\n",
    "\n",
    "    tokenizer.add_tokens(additional_special_tokens)\n",
    "    #bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')    \n",
    "    data_files = ['train_rr_data.csv', 'dev_rr_data.csv' , 'test_rr_data.csv']\n",
    "\n",
    "    \n",
    "                                    \n",
    "    \n",
    "    summary_data = make_data(tokenizer, SummaryDataModule, data_type = 'robo', path = '/Users/sanjana', files = data_files, max_len = 1024)\n",
    "    #print(summary_data.train)\n",
    "    summary_data.setup()\n",
    "    it = summary_data.val_dataloader()\n",
    "    batches = iter(it)\n",
    "    batch = next(batches)\n",
    "    \n",
    "    def print_pico(batch):\n",
    "        content_input_ids = batch[0] if len(batch) >1 else None\n",
    "        content_attention_masks = batch[1] if len(batch) >1 else None\n",
    "        print(\"CONTENT\")\n",
    "        print(\" \".join([tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in content_input_ids]))\n",
    "        print(content_attention_masks)\n",
    "\n",
    "    print_pico(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/sanjana/summarization/datasets/train_rr_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dict = {}\n",
    "content_keys = ['interventions_mesh', 'outcomes_mesh', 'punchline_effect']\n",
    "surface_keys = ['population', 'punchline_text']\n",
    "for _, row in list(df.iterrows())[:5]:\n",
    "    content_val = []\n",
    "    num_items = len(eval(row[content_keys[0]]))\n",
    "    row_contents = {}\n",
    "    for idx in range(0, num_items):\n",
    "        idx_vals_content = [\"<%s> \"%k+ eval(row[k])[idx] + \" </%s>\"%k for k in content_keys]\n",
    "        content_sent = \" \".join(idx_vals_content)\n",
    "        content_val.append( \"<content> \"+ content_sent + \" </content>\")\n",
    "        \n",
    "    for surface_k in surface_keys:\n",
    "        row_surface_k = eval(row[surface_k])\n",
    "        row_surface_k = \" \".join([\"<surface> <%s> \"%surface_k+ each + \" </%s> </surface>\"%surface_k for each in row_surface_k])\n",
    "        row_contents[surface_k] =  row_surface_k\n",
    "    \n",
    "    row_content_sent = \" \".join(content_val)\n",
    "    row_contents['content'] = row_content_sent\n",
    "    \n",
    "    \n",
    "    print(row_contents.keys())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc6a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
