{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b4f8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BartTokenizer, BartForCausalLM, BartForConditionalGeneration, BeamSearchScorer, LogitsProcessorList, MinLengthLogitsProcessor, TopKLogitsWarper, TemperatureLogitsWarper, BartModel\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import BartTokenizer, BartForCausalLM, BartForConditionalGeneration, BeamSearchScorer, LogitsProcessorList, MinLengthLogitsProcessor, TopKLogitsWarper, TemperatureLogitsWarper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c0e117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(tokenizer, df, surface_keys, content_keys, targets, max_length=1024, pad_to_max_length=True, return_tensors=\"pt\"):\n",
    "\n",
    "    \n",
    "    encoded_sentences = {}\n",
    "\n",
    "    target_ids = []\n",
    "    \n",
    "    def run_bart(snippet):\n",
    "        encoded_dict = tokenizer(\n",
    "          snippet,\n",
    "          max_length=max_length,\n",
    "          padding=\"max_length\" if pad_to_max_length else None,\n",
    "          truncation=True,\n",
    "          return_tensors=return_tensors,\n",
    "          add_prefix_space = True\n",
    "        )\n",
    "        return encoded_dict\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        content_val = []\n",
    "        num_items = len(eval(row[content_keys[0]]))\n",
    "        row_contents = {}\n",
    "        for idx in range(0, num_items):\n",
    "            idx_vals_content = [\"<%s> \"%k+ eval(row[k])[idx] + \" </%s>\"%k for k in content_keys]\n",
    "            content_sent = \" \".join(idx_vals_content)\n",
    "            content_val.append( \"<content> \"+ content_sent + \" </content>\")\n",
    "\n",
    "        for surface_k in surface_keys:\n",
    "            row_surface_k = eval(row[surface_k])\n",
    "            row_surface_k = \" \".join([\"<surface> <%s> \"%surface_k+ each + \" </%s> </surface>\"%surface_k for each in row_surface_k])\n",
    "            row_contents[surface_k] =  row_surface_k\n",
    "\n",
    "        row_content_sent = \" \".join(content_val)\n",
    "        row_contents['content'] = row_content_sent\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for key, key_sent in row_contents.items():\n",
    "            id_key = '%s_ids'%key\n",
    "            attention_mask_key = '%s_attention_masks'%key\n",
    "            if id_key not in encoded_sentences:\n",
    "                encoded_sentences[id_key] = []\n",
    "                encoded_sentences[attention_mask_key] = []\n",
    "                \n",
    "            sentence_encoding = run_bart(key_sent.strip())\n",
    "            \n",
    "            encoded_sentences[id_key].append(sentence_encoding['input_ids'])\n",
    "            encoded_sentences[attention_mask_key].append(sentence_encoding['attention_mask'])\n",
    "            \n",
    "    for tgt_sentence in targets:\n",
    "        encoded_dict = tokenizer(\n",
    "              tgt_sentence,\n",
    "              max_length=max_length,\n",
    "              padding=\"max_length\" if pad_to_max_length else None,\n",
    "              truncation=True,\n",
    "              return_tensors=return_tensors,\n",
    "              add_prefix_space = True\n",
    "        )\n",
    "        # Shift the target ids to the right\n",
    "        #shifted_target_ids = shift_tokens_right(encoded_dict['input_ids'], tokenizer.pad_token_id)\n",
    "        target_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "    for key in list(encoded_sentences.keys()):\n",
    "        print(len(encoded_sentences[key]))\n",
    "        encoded_sentences[key] = torch.cat(encoded_sentences[key], dim = 0)\n",
    "        print(encoded_sentences[key].shape)\n",
    "        \n",
    "    target_ids = torch.cat(target_ids, dim = 0)\n",
    "    encoded_sentences['labels'] = target_ids\n",
    "    print(encoded_sentences['labels'].shape)\n",
    "        \n",
    "        \n",
    "\n",
    "    return encoded_sentences\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "656e0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, keys):\n",
    "    for key in keys:\n",
    "        df = df[df[key] != \"['']\"]\n",
    "    return df\n",
    "\n",
    "class SummaryDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, data_files, batch_size, num_examples = 20000 , max_len = 1024, flatten_studies = False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_files = data_files\n",
    "        self.batch_size = batch_size\n",
    "        self.num_examples = num_examples\n",
    "        self.max_len = max_len\n",
    "        self.flatten_studies = flatten_studies\n",
    "\n",
    "    # Loads and splits the data into training, validation and test sets with a 60/20/20 split\n",
    "    def prepare_data(self):\n",
    "        self.train = pd.read_csv(self.data_files[0])\n",
    "        self.validate = pd.read_csv(self.data_files[1])\n",
    "        self.test = pd.read_csv(self.data_files[2])\n",
    "        preprocess_keys = ['population', 'interventions', 'outcomes', 'SummaryConclusions','punchline_text', 'punchline_effect' ]\n",
    "        self.train = preprocess_df(self.train, preprocess_keys)\n",
    "        self.validate = preprocess_df(self.validate, preprocess_keys)\n",
    "        self.test = preprocess_df(self.test, preprocess_keys)\n",
    "\n",
    "    def setup(self):\n",
    "        self.train = encode_sentences(self.tokenizer, \n",
    "                                      self.train,\n",
    "                                        ['population', 'punchline_text'],\n",
    "                                        ['interventions_mesh', 'outcomes_mesh', 'punchline_effect'], \n",
    "                                        self.train['SummaryConclusions'],\n",
    "                                        max_length = self.max_len)\n",
    "        \n",
    "        self.validate = encode_sentences(self.tokenizer, \n",
    "                                      self.validate,\n",
    "                                        ['population', 'punchline_text'],\n",
    "                                        ['interventions_mesh', 'outcomes_mesh', 'punchline_effect'], \n",
    "                                        self.validate['SummaryConclusions'],\n",
    "                                        max_length = self.max_len)\n",
    "        self.test = encode_sentences(self.tokenizer, \n",
    "                                      self.test,\n",
    "                                        ['population', 'punchline_text'],\n",
    "                                        ['interventions_mesh', 'outcomes_mesh', 'punchline_effect'], \n",
    "                                        self.test['SummaryConclusions'],\n",
    "                                        max_length = self.max_len)\n",
    "        \n",
    "    def train_dataloader(self, data_type = 'robo'):\n",
    "        #dataset = TensorDataset\n",
    "        dataset = TensorDataset(self.train['content_ids'], self.train['content_attention_masks'],\n",
    "                                self.train['population_ids'], self.train['population_attention_masks'],\n",
    "                                self.train['punchline_text_ids'], self.train['punchline_text_attention_masks'],\n",
    "                                    self.train['labels'])\n",
    "        #dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n",
    "        train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n",
    "        return train_data\n",
    "\n",
    "    def val_dataloader(self, data_type = 'robo'):\n",
    "        dataset = TensorDataset(self.validate['content_ids'], self.validate['content_attention_masks'],\n",
    "                                self.validate['population_ids'], self.validate['population_attention_masks'],\n",
    "                                self.validate['punchline_text_ids'], self.validate['punchline_text_attention_masks'],\n",
    "                                    self.validate['labels'])\n",
    "        val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n",
    "        return val_data\n",
    "\n",
    "    def test_dataloader(self, data_type = 'robo'):\n",
    "        #print(self.test['punchline_text_ids'])\n",
    "        dataset = TensorDataset(self.test['content_ids'], self.test['content_attention_masks'],\n",
    "                                self.test['population_ids'], self.test['population_attention_masks'],\n",
    "                                self.test['punchline_text_ids'], self.test['punchline_text_attention_masks'],\n",
    "                                    self.train['labels'])\n",
    "        test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "216774e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sanjana/summarization/datasets/train_rr_data.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5f30a40b8de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0msummary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummaryDataModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'robo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/sanjana'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#print(summary_data.train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0msummary_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-e45e7f4ae187>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m                                         \u001b[0;34m[\u001b[0m\u001b[0;34m'interventions_mesh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outcomes_mesh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'punchline_effect'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SummaryConclusions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                         max_length = self.max_len)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         self.validate = encode_sentences(self.tokenizer, \n",
      "\u001b[0;32m<ipython-input-49-14e093c26ebb>\u001b[0m in \u001b[0;36mencode_sentences\u001b[0;34m(tokenizer, df, surface_keys, content_keys, targets, max_length, pad_to_max_length, return_tensors)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mencoded_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattention_mask_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0msentence_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_bart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mencoded_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-14e093c26ebb>\u001b[0m in \u001b[0;36mrun_bart\u001b[0;34m(snippet)\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           \u001b[0madd_prefix_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2346\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2348\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2349\u001b[0m             )\n\u001b[1;32m   2350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2416\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m         )\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             )\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     (\n\u001b[1;32m    355\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     )\n\u001b[1;32m    358\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     (\n\u001b[1;32m    355\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     )\n\u001b[1;32m    358\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_token\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbpe_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2.py\u001b[0m in \u001b[0;36mbpe\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def make_data(tokenizer, SummaryDataModule,  data_type = 'robo', path = '/Users/sanjana', files = ['robo_train_sep.csv', 'robo_dev_sep.csv', 'robo_test_sep.csv'], max_len = 256):\n",
    "    if data_type == 'robo':\n",
    "        train_file = path + '/summarization/datasets/%s'%(files[0])\n",
    "        dev_file = path + '/summarization/datasets/%s'%(files[1])\n",
    "        test_file = path + '/summarization/datasets/%s'%(files[2])\n",
    "\n",
    "    print(train_file)\n",
    "    data_files = [train_file, dev_file, test_file]\n",
    "    summary_data = SummaryDataModule(tokenizer, data_files = data_files,  batch_size = 1, max_len = max_len, flatten_studies = True)\n",
    "    summary_data.prepare_data()\n",
    "    \n",
    "    assert(len(summary_data.train) > 10)\n",
    "    return summary_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    additional_special_tokens = [\"<sep>\", \"<study>\", \"</study>\",\n",
    "            \"<outcomes_mesh>\", \"</outcomes_mesh>\",\n",
    "            \"<punchline_text>\", \"</punchline_text>\",\n",
    "            \"<population_mesh>\", \"</population_mesh>\",\n",
    "            \"<interventions_mesh>\", \"</interventions_mesh>\",\n",
    "            \"<punchline_effect>\", \"</punchline_effect>\"]\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', bos_token=\"<s>\", \n",
    "                                                    eos_token=\"</s>\", \n",
    "                                                    pad_token = \"<pad>\")\n",
    "\n",
    "    tokenizer.add_tokens(additional_special_tokens)\n",
    "    #bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')    \n",
    "    data_files = ['train_rr_data.csv', 'dev_rr_data.csv' , 'test_rr_data.csv']\n",
    "\n",
    "    \n",
    "                                    \n",
    "    \n",
    "    summary_data = make_data(tokenizer, SummaryDataModule, data_type = 'robo', path = '/Users/sanjana', files = data_files, max_len = 1024)\n",
    "    #print(summary_data.train)\n",
    "    summary_data.setup()\n",
    "    it = summary_data.val_dataloader()\n",
    "    batches = iter(it)\n",
    "    batch = next(batches)\n",
    "    \n",
    "    def print_pico(batch):\n",
    "        content_input_ids = batch[0] if len(batch) >1 else None\n",
    "        content_attention_masks = batch[1] if len(batch) >1 else None\n",
    "        print(\"CONTENT\")\n",
    "        print(\" \".join([tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in content_input_ids]))\n",
    "        print(content_attention_masks)\n",
    "\n",
    "    print_pico(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/sanjana/summarization/datasets/train_rr_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbe9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dict = {}\n",
    "content_keys = ['interventions_mesh', 'outcomes_mesh', 'punchline_effect']\n",
    "surface_keys = ['population', 'punchline_text']\n",
    "for _, row in list(df.iterrows())[:5]:\n",
    "    content_val = []\n",
    "    num_items = len(eval(row[content_keys[0]]))\n",
    "    row_contents = {}\n",
    "    for idx in range(0, num_items):\n",
    "        idx_vals_content = [\"<%s> \"%k+ eval(row[k])[idx] + \" </%s>\"%k for k in content_keys]\n",
    "        content_sent = \" \".join(idx_vals_content)\n",
    "        content_val.append( \"<content> \"+ content_sent + \" </content>\")\n",
    "        \n",
    "    for surface_k in surface_keys:\n",
    "        row_surface_k = eval(row[surface_k])\n",
    "        row_surface_k = \" \".join([\"<surface> <%s> \"%surface_k+ each + \" </%s> </surface>\"%surface_k for each in row_surface_k])\n",
    "        row_contents[surface_k] =  row_surface_k\n",
    "    \n",
    "    row_content_sent = \" \".join(content_val)\n",
    "    row_contents['content'] = row_content_sent\n",
    "    \n",
    "    \n",
    "    print(row_contents.keys())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2490a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
